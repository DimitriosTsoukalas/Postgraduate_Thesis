{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Feature\n",
    "\n",
    "The current experiment will focus on Description Feature. It will try to bring the values of description feature into a more friendly for processing content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 - Merged (skip that)\n",
    "This experiment includes all descriptions of one picture, gathered in one cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2c1fe3c992fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#Skip that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"merged.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Skip that\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"merged.csv\")\n",
    "df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 - Unique\n",
    "\n",
    "This experiment includes separate rows (each for every description) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Phase 1 - Import and print the first 10 rows of the description feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    'A female with a regular face[comma] longish h...\n",
      "1                                       'average guy',\n",
      "2                                'Guy with long hair',\n",
      "3      'Young male[comma] Dark blondish brown hair. ',\n",
      "4    'This is a young Asian woman who has dark hair...\n",
      "5    'Female[comma] short brown hair, wearing black...\n",
      "6    'This man appears to be white[comma] with a Eu...\n",
      "7    'slightly overweight young white male with a b...\n",
      "8    'This seems to be a middle aged Asian woman wi...\n",
      "9    'She is a student in psychology[comma] but als...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"unique.csv\")\n",
    "df1 = df[\"0\"]\n",
    "print df1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 2 - Strip from the unecessary [comma]\n",
    "\n",
    "In the above 10 examples captured, we can see that in row numbers 3,5,6 and 9 the word [comma] is included. This may affect our performance of capturing specific words. Therefore we will remove that uncessary word, split words into tokens and turn everything into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allsentences = []\n",
    "\n",
    "for items in df1:\n",
    "    sentence = []\n",
    "    for words in items.split():\n",
    "        #print words\n",
    "        pattern = re.search(r'(.*)(\\[comma])',words)\n",
    "        if pattern:\n",
    "            sentence.append(pattern.group(1).lower())\n",
    "        else:\n",
    "            sentence.append(words.lower())\n",
    "    allsentences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'a\", 'female', 'with', 'a', 'regular', 'face', 'longish', 'hair,', 'some', 'lines', 'around', 'her', 'neck', 'suggesting', 'she', 'might', 'be', 'older', 'than', 'she', 'seems', 'to', 'be.', 'she', 'is', 'wearing', 'a', 'black', 'shirt', 'and', 'has', 'a', 'hint', 'of', 'a', 'smile', 'on', 'her', 'face', 'and', 'in', 'her', \"eyes.',\"]\n",
      "[\"'average\", \"guy',\"]\n",
      "[\"'guy\", 'with', 'long', \"hair',\"]\n",
      "[\"'young\", 'male', 'dark', 'blondish', 'brown', 'hair.', \"',\"]\n",
      "[\"'this\", 'is', 'a', 'young', 'asian', 'woman', 'who', 'has', 'dark', 'hair', 'and', 'eyes', 'full', 'lips,', 'and', 'a', 'broad', \"nose.',\"]\n",
      "[\"'female\", 'short', 'brown', 'hair,', 'wearing', 'black', 'framed', \"glasses.',\"]\n",
      "[\"'this\", 'man', 'appears', 'to', 'be', 'white', 'with', 'a', 'european', 'descent.', 'he', 'has', 'short,', 'dirty-blond', \"hair.',\"]\n",
      "[\"'slightly\", 'overweight', 'young', 'white', 'male', 'with', 'a', 'black', 'shirt', 'and', 'plaid', 'shirt', \"underneath',\"]\n",
      "[\"'this\", 'seems', 'to', 'be', 'a', 'middle', 'aged', 'asian', 'woman', 'with', 'dark', 'hair', 'and', 'dark', 'eyes.', \"',\"]\n",
      "[\"'she\", 'is', 'a', 'student', 'in', 'psychology', 'but', 'also', 'has', 'to', 'work', 'part', \"time.',\"]\n"
     ]
    }
   ],
   "source": [
    "for items in range (10):\n",
    "    print allsentences[items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional Phase - make the words one consecutive sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lista=  []\n",
    "\n",
    "for items in allsentences:\n",
    "    captured = \" \".join(items)\n",
    "    lista.append(captured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Phase 3 - Remove stop words and remove unecessary characters (like: ' or , or .)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listb = []\n",
    "\n",
    "for items in lista:\n",
    "    summing = []\n",
    "    items = items.split()\n",
    "    for words in items:\n",
    "        words = words.strip(\"\\',.\")\n",
    "        #print words\n",
    "        if words not in stopwords.words(\"english\"):\n",
    "            summing.append(words)\n",
    "    listb.append(summing)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'regular', 'face', 'longish', 'hair', 'lines', 'around', 'neck', 'suggesting', 'might', 'older', 'seems', 'wearing', 'black', 'shirt', 'hint', 'smile', 'face', 'eyes']\n",
      "['average', 'guy']\n",
      "['guy', 'long', 'hair']\n",
      "['young', 'male', 'dark', 'blondish', 'brown', 'hair', '']\n",
      "['young', 'asian', 'woman', 'dark', 'hair', 'eyes', 'full', 'lips', 'broad', 'nose']\n",
      "['female', 'short', 'brown', 'hair', 'wearing', 'black', 'framed', 'glasses']\n",
      "['man', 'appears', 'white', 'european', 'descent', 'short', 'dirty-blond', 'hair']\n",
      "['slightly', 'overweight', 'young', 'white', 'male', 'black', 'shirt', 'plaid', 'shirt', 'underneath']\n",
      "['seems', 'middle', 'aged', 'asian', 'woman', 'dark', 'hair', 'dark', 'eyes', '']\n",
      "['student', 'psychology', 'also', 'work', 'part', 'time']\n"
     ]
    }
   ],
   "source": [
    "for items in range(10):\n",
    "    print listb[items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 4 - Join words into one string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female regular face longish hair lines around neck suggesting might older seems wearing black shirt hint smile face eyes\n",
      "average guy\n",
      "guy long hair\n",
      "young male dark blondish brown hair \n",
      "young asian woman dark hair eyes full lips broad nose\n",
      "female short brown hair wearing black framed glasses\n",
      "man appears white european descent short dirty-blond hair\n",
      "slightly overweight young white male black shirt plaid shirt underneath\n",
      "seems middle aged asian woman dark hair dark eyes \n",
      "student psychology also work part time\n"
     ]
    }
   ],
   "source": [
    "listc=  []\n",
    "\n",
    "for items in listb:\n",
    "    captured = \" \".join(items)\n",
    "    listc.append(captured)\n",
    "    \n",
    "for items in range (10):\n",
    "    print listc[items]\n",
    "\n",
    "#Comment: Although the sentences make less sense, it is important to highlighten that the words are now ready to be captured as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 5 - Provide results. Images with short description, containing only words that matter. Now first level of preprocessing achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'IMG_888\n",
      "female regular face longish hair lines around neck suggesting might older seems wearing black shirt hint smile face eyes\n",
      "\n",
      "'IMG_038\n",
      "average guy\n",
      "\n",
      "'IMG_963\n",
      "guy long hair\n",
      "\n",
      "'IMG_076\n",
      "young male dark blondish brown hair \n",
      "\n",
      "'IMG_893\n",
      "young asian woman dark hair eyes full lips broad nose\n",
      "\n",
      "'IMG_756\n",
      "female short brown hair wearing black framed glasses\n",
      "\n",
      "'IMG_076\n",
      "man appears white european descent short dirty-blond hair\n",
      "\n",
      "'IMG_800\n",
      "slightly overweight young white male black shirt plaid shirt underneath\n",
      "\n",
      "'IMG_975\n",
      "seems middle aged asian woman dark hair dark eyes \n",
      "\n",
      "'IMG_989\n",
      "student psychology also work part time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for items in range (10):\n",
    "    print df[\"3\"][items]\n",
    "    print listc[items]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phase 6 - Conclusion of experiment\n",
    "\n",
    "1. We managed to bring the database into a more \"friendly\" level. Words can now be accessed in an easier way.\n",
    "2. Next step involves on testing topic modeling, chunking and regular expression appliance in order to mine important information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
