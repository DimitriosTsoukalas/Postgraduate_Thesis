{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering\n",
    "\n",
    "The K-means algorithm has been tested in the question feature. There were two preprocessing routes taken before deciding the optimal one and both of them are presented. Then the results are presented after applying the algorithm to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here there is a sample of questions from the question feature:\n",
      "\n",
      "0                   Is the person wearing glasses?\n",
      "1                      Does the person have bangs?\n",
      "2                      Does the person have bangs?\n",
      "3                Does the person have blonde hair?\n",
      "4                 Does the person have short hair?\n",
      "5            Does the person ave her hair tied up?\n",
      "6    Does the person have her hair in a ponytail? \n",
      "7          Does the person have her hair in a bun?\n",
      "8                    Does the person have glasses?\n",
      "9                      Does the person have bangs?\n",
      "Name: Question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"questions1.csv\")\n",
    "df1 = df[\"Question\"]\n",
    "\n",
    "print \"Here there is a sample of questions from the question feature:\"\n",
    "print\n",
    "print df1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Route (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person wearing glasses?', 'person bangs?', 'person bangs?', 'person blonde hair?', 'person short hair?', 'person ave hair tied up?', 'person hair ponytail?', 'person hair bun?', 'person glasses?', 'person bangs?']\n"
     ]
    }
   ],
   "source": [
    "allsentences = []\n",
    "\n",
    "for items in df1:\n",
    "    sentence = []\n",
    "    for words in items.split():\n",
    "        #print words\n",
    "        pattern = re.search(r'(.*)(\\[comma])',words)\n",
    "        if pattern:\n",
    "            sentence.append(pattern.group(1).lower())\n",
    "        else:\n",
    "            sentence.append(words.lower())\n",
    "    allsentences.append(sentence)\n",
    "\n",
    "lista=  []\n",
    "\n",
    "for items in allsentences:\n",
    "    captured = \" \".join(items)\n",
    "    lista.append(captured)\n",
    "    \n",
    "listb = []\n",
    "\n",
    "for items in lista:\n",
    "    summing = []\n",
    "    items = items.split()\n",
    "    for words in items:\n",
    "        words = words.strip(\"\\',.\")\n",
    "        #print words\n",
    "        if words not in stopwords.words(\"english\"):\n",
    "            summing.append(words)\n",
    "    listb.append(summing)\n",
    "    \n",
    "listc=  []\n",
    "\n",
    "for items in listb:\n",
    "    captured = \" \".join(items)\n",
    "    listc.append(captured)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing route (1) \n",
    "Optional - Removing of some specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glasses\n",
      "bangs\n",
      "bangs\n",
      "blonde hair\n",
      "short hair\n",
      "ave hair tied up\n",
      "hair ponytail\n",
      "hair bun\n",
      "glasses\n",
      "bangs\n"
     ]
    }
   ],
   "source": [
    "listd = []\n",
    "\n",
    "for items in listc:\n",
    "    temporar = []\n",
    "    #print items.split()\n",
    "    for words in items.split():\n",
    "        if \"person\" in words:\n",
    "            continue\n",
    "        elif \"wearing\" in words:\n",
    "            continue\n",
    "        elif \"wear\" in words:\n",
    "            continue\n",
    "        else:\n",
    "            temporar.append(words)\n",
    "    listd.append(temporar)\n",
    "    \n",
    "liste=  []\n",
    "\n",
    "\n",
    "for items in listd:\n",
    "    captured = \" \".join(items).strip(\"?\")\n",
    "    liste.append(captured)\n",
    "    \n",
    "for items in range (10):\n",
    "    print liste[items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing route (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in liste:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print 'there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Process Tf-idf and document similarity\n",
    "\n",
    "This process vectorises the information that was prerpocessed earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.95 s\n",
      "(3237, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=3,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(listc) #fit the vectorizer to synopses\n",
    "\n",
    "#print(tfidf_matrix.shape)\n",
    "\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying K means\n",
    "\n",
    "Here the algorithm is applied using 10 clusters. The thesis, examined also examples with 2,4,6 and 8 clusters as well. The document provides the full picture and some further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presenting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    942\n",
       "2    938\n",
       "0    390\n",
       "3    367\n",
       "4    339\n",
       "5    244\n",
       "6     14\n",
       "7      2\n",
       "8      1\n",
       "Name: Cluster, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = {'Question': listc, 'Cluster': clusters}\n",
    "frame = pd.DataFrame(product, index = [clusters] , columns = [\"Cluster\",'Question'])\n",
    "frame['Cluster'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
